# Task: Create Production Risk Estimates Exploration Notebook

**Status**: TODO  
**Priority**: HIGH - Validates production parameter optimization results  
**Estimated Time**: 3-4 hours  
**Dependencies**: Production parameter optimization complete (âœ“)

## Overview

Create a comprehensive Jupyter notebook that explores and visualizes the risk estimates generated by the production-optimized parameters. This notebook will demonstrate the production API usage and validate that the optimization produced meaningful results.

## Objectives

1. **Demonstrate Production API**: Show how to use `OptimizedRiskEstimator` with real parameters
2. **Explore Risk Estimates**: Analyze volatility and correlation estimates for all exposures
3. **Validate Results**: Ensure estimates are reasonable and well-behaved
4. **Visualize Insights**: Create professional visualizations of risk decomposition
5. **Export for Portfolio Use**: Generate files ready for portfolio optimization

## Notebook Structure

### 1. Setup and Production API Demo (30 min)
```python
# notebooks/production_risk_estimates_exploration.ipynb

# Cell 1: Introduction
"""
# Production Risk Estimates Exploration

This notebook explores the risk estimates generated using the production-optimized 
parameters from July 10, 2025. We'll examine:

1. Volatility estimates for all 14 exposures
2. Correlation matrix structure and properties
3. Risk decomposition (risk premium vs uncompensated components)
4. Comparison with simple historical estimates
5. Production API usage patterns

**Key Innovation**: These estimates use parameters optimized specifically for 
risk premium forecasting, not total returns.
"""

# Cell 2: Setup and Load Production Estimator
from src.optimization import OptimizedRiskEstimator
from datetime import datetime
import pandas as pd
import numpy as np

# Initialize with production parameters (automatically loaded)
estimator = OptimizedRiskEstimator()

# Show parameter summary
param_summary = estimator.get_parameter_summary()
print("ðŸ“Š Production Parameters Loaded:")
print(f"  Volatility: {param_summary['volatility']['method']} method, "
      f"{param_summary['volatility']['lookback_days']} day lookback")
print(f"  Correlation: {param_summary['correlation']['method']} method, "
      f"{param_summary['correlation']['lookback_days']} day lookback")
print(f"  Expected Returns: {param_summary['returns']['method']} method, "
      f"{param_summary['returns']['lookback_days']} day lookback")
```

### 2. Individual Exposure Risk Estimates (45 min)

```python
# Cell 3: Generate Risk Estimates for All Exposures
estimation_date = datetime.now()
exposure_ids = [
    'us_large_equity', 'us_small_equity', 'intl_developed_large_equity',
    'intl_developed_small_equity', 'emerging_equity', 'real_estate',
    'commodities', 'gold', 'tips', 'short_ust', 'broad_ust',
    'dynamic_global_bonds', 'factor_style_equity', 'factor_style_other'
]

# Collect individual estimates
risk_estimates = {}
for exp_id in exposure_ids:
    try:
        estimate = estimator.estimate_risk(
            exposure_id=exp_id,
            estimation_date=estimation_date,
            include_decomposition=True  # Get component breakdown
        )
        risk_estimates[exp_id] = estimate
    except Exception as e:
        print(f"âš ï¸ Failed to estimate {exp_id}: {e}")

# Create summary DataFrame
summary_data = []
for exp_id, estimate in risk_estimates.items():
    summary_data.append({
        'exposure_id': exp_id,
        'risk_premium_vol': estimate.risk_premium_volatility,
        'total_vol': estimate.total_volatility,
        'inflation_vol': estimate.component_volatilities.get('inflation', 0),
        'rf_vol': estimate.component_volatilities.get('real_rf', 0),
        'rp_percentage': (estimate.risk_premium_volatility / 
                         estimate.total_volatility * 100)
    })

risk_summary_df = pd.DataFrame(summary_data)
```

```python
# Cell 4: Volatility Analysis Visualization
import matplotlib.pyplot as plt
import seaborn as sns

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Risk Premium vs Total Volatility Comparison
ax = axes[0, 0]
x = np.arange(len(risk_summary_df))
width = 0.35

ax.bar(x - width/2, risk_summary_df['risk_premium_vol'] * 100, 
       width, label='Risk Premium Vol', alpha=0.8)
ax.bar(x + width/2, risk_summary_df['total_vol'] * 100, 
       width, label='Total Return Vol', alpha=0.8)

ax.set_xlabel('Exposure')
ax.set_ylabel('Annualized Volatility (%)')
ax.set_title('Risk Premium vs Total Return Volatility')
ax.set_xticks(x)
ax.set_xticklabels(risk_summary_df['exposure_id'], rotation=45, ha='right')
ax.legend()
ax.grid(True, alpha=0.3)

# 2. Risk Premium as Percentage of Total
ax = axes[0, 1]
ax.bar(x, risk_summary_df['rp_percentage'], alpha=0.8, 
       color='green' if risk_summary_df['rp_percentage'].mean() > 90 else 'orange')
ax.axhline(y=100, color='red', linestyle='--', alpha=0.5)
ax.set_xlabel('Exposure')
ax.set_ylabel('Risk Premium as % of Total')
ax.set_title('Risk Premium Percentage of Total Volatility')
ax.set_xticks(x)
ax.set_xticklabels(risk_summary_df['exposure_id'], rotation=45, ha='right')
ax.grid(True, alpha=0.3)

# 3. Volatility by Asset Class
ax = axes[1, 0]
# Group by asset class (you'll need to add this categorization)
asset_classes = {
    'Equity': ['us_large_equity', 'us_small_equity', 'intl_developed_large_equity', 
               'intl_developed_small_equity', 'emerging_equity'],
    'Fixed Income': ['short_ust', 'broad_ust', 'dynamic_global_bonds'],
    'Real Assets': ['real_estate', 'commodities', 'gold', 'tips'],
    'Alternatives': ['factor_style_equity', 'factor_style_other']
}

# Create box plot by asset class
data_for_box = []
for asset_class, exposures in asset_classes.items():
    for exp in exposures:
        if exp in risk_summary_df['exposure_id'].values:
            vol = risk_summary_df[risk_summary_df['exposure_id'] == exp]['risk_premium_vol'].values[0]
            data_for_box.append({'Asset Class': asset_class, 'Volatility': vol * 100})

box_df = pd.DataFrame(data_for_box)
sns.boxplot(data=box_df, x='Asset Class', y='Volatility', ax=ax)
ax.set_ylabel('Risk Premium Volatility (%)')
ax.set_title('Volatility Distribution by Asset Class')

# 4. Component Breakdown for Select Exposures
ax = axes[1, 1]
select_exposures = ['us_large_equity', 'broad_ust', 'commodities', 'real_estate']
component_data = []

for exp in select_exposures:
    if exp in risk_estimates:
        est = risk_estimates[exp]
        component_data.append({
            'Exposure': exp,
            'Risk Premium': est.risk_premium_volatility * 100,
            'Inflation': est.component_volatilities.get('inflation', 0) * 100,
            'Real RF': est.component_volatilities.get('real_rf', 0) * 100
        })

comp_df = pd.DataFrame(component_data)
comp_df.set_index('Exposure')[['Risk Premium', 'Inflation', 'Real RF']].plot(
    kind='bar', stacked=True, ax=ax
)
ax.set_ylabel('Volatility (%)')
ax.set_title('Risk Decomposition for Select Exposures')
ax.legend(title='Component')

plt.tight_layout()
plt.show()
```

### 3. Correlation Matrix Analysis (45 min)

```python
# Cell 5: Generate and Analyze Correlation Matrix
# Get correlation matrix
correlation_matrix = estimator.get_correlation_matrix(
    exposure_ids=exposure_ids,
    estimation_date=estimation_date
)

print(f"ðŸ“Š Correlation Matrix Properties:")
print(f"  Shape: {correlation_matrix.shape}")
print(f"  Symmetric: {np.allclose(correlation_matrix, correlation_matrix.T)}")
print(f"  Positive Definite: {np.all(np.linalg.eigvals(correlation_matrix) > 0)}")
print(f"  Condition Number: {np.linalg.cond(correlation_matrix):.2f}")

# Check eigenvalues
eigenvalues = np.linalg.eigvals(correlation_matrix)
print(f"\nðŸ“ˆ Eigenvalue Analysis:")
print(f"  Largest: {eigenvalues.max():.4f}")
print(f"  Smallest: {eigenvalues.min():.4f}")
print(f"  Ratio: {eigenvalues.max() / eigenvalues.min():.2f}")
```

```python
# Cell 6: Correlation Matrix Visualization
fig, axes = plt.subplots(2, 2, figsize=(16, 14))

# 1. Full Correlation Heatmap
ax = axes[0, 0]
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
sns.heatmap(correlation_matrix, 
            mask=mask,
            cmap='RdBu_r', 
            center=0,
            vmin=-1, vmax=1,
            square=True,
            linewidths=0.5,
            cbar_kws={"shrink": 0.8},
            ax=ax)
ax.set_title('Risk Premium Correlation Matrix (Lower Triangle)')

# 2. Correlation Distribution
ax = axes[0, 1]
# Extract lower triangle (excluding diagonal)
lower_triangle = correlation_matrix[np.tril_indices_from(correlation_matrix, k=-1)]
ax.hist(lower_triangle, bins=30, alpha=0.7, edgecolor='black')
ax.axvline(x=lower_triangle.mean(), color='red', linestyle='--', 
           label=f'Mean: {lower_triangle.mean():.3f}')
ax.set_xlabel('Correlation')
ax.set_ylabel('Frequency')
ax.set_title('Distribution of Pairwise Correlations')
ax.legend()
ax.grid(True, alpha=0.3)

# 3. Average Correlation by Asset
ax = axes[1, 0]
avg_correlations = pd.DataFrame(correlation_matrix, 
                               index=exposure_ids, 
                               columns=exposure_ids).mean(axis=1)
avg_correlations.sort_values().plot(kind='barh', ax=ax)
ax.set_xlabel('Average Correlation with Other Assets')
ax.set_title('Average Correlations by Exposure')
ax.grid(True, alpha=0.3)

# 4. Correlation Clusters
ax = axes[1, 1]
# Simple clustering visualization
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import squareform

# Convert correlation to distance
distance_matrix = 1 - correlation_matrix
condensed_distances = squareform(distance_matrix)
linkage_matrix = linkage(condensed_distances, method='average')

dendrogram(linkage_matrix, 
          labels=exposure_ids,
          ax=ax,
          orientation='left')
ax.set_xlabel('Distance (1 - Correlation)')
ax.set_title('Hierarchical Clustering of Exposures')

plt.tight_layout()
plt.show()
```

### 4. Production API Demonstration (30 min)

```python
# Cell 7: Complete Production Workflow
"""
## Production API Usage Examples

Demonstrating the simple, clean API for portfolio optimization inputs.
"""

# Example 1: Get everything for portfolio optimization
print("ðŸ“¦ Example 1: Complete Portfolio Optimization Inputs")
print("-" * 50)

portfolio_exposures = ['us_large_equity', 'broad_ust', 'commodities', 'real_estate']
optimization_inputs = estimator.get_optimization_ready_inputs(
    exposure_ids=portfolio_exposures,
    estimation_date=estimation_date
)

print(f"âœ… Received optimization inputs:")
print(f"  - Covariance Matrix: {optimization_inputs['covariance_matrix'].shape}")
print(f"  - Expected Returns: {len(optimization_inputs['expected_returns'])}")
print(f"  - Volatilities: {len(optimization_inputs['volatilities'])}")
print(f"\nSample volatilities (annualized %):")
for exp, vol in optimization_inputs['volatilities'].items():
    print(f"  {exp}: {vol*100:.2f}%")
```

```python
# Cell 8: Risk Decomposition Deep Dive
"""
## Risk Decomposition Analysis

Understanding the components of total return volatility.
"""

# Select a few exposures for detailed analysis
detailed_exposures = ['us_large_equity', 'broad_ust', 'commodities']

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for idx, exp_id in enumerate(detailed_exposures):
    ax = axes[idx]
    estimate = risk_estimates[exp_id]
    
    # Create pie chart of variance contributions
    rp_var = estimate.risk_premium_volatility ** 2
    inf_var = estimate.component_volatilities.get('inflation', 0) ** 2
    rf_var = estimate.component_volatilities.get('real_rf', 0) ** 2
    
    # Note: These don't add up directly due to correlations
    total_var = estimate.total_volatility ** 2
    
    # Approximate variance contributions
    sizes = [rp_var/total_var * 100, inf_var/total_var * 100, rf_var/total_var * 100]
    labels = ['Risk Premium', 'Inflation', 'Real Risk-Free']
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    
    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
    ax.set_title(f'{exp_id}\\nVariance Decomposition')

plt.tight_layout()
plt.show()
```

### 5. Comparison with Naive Estimates (30 min)

```python
# Cell 9: Compare with Simple Historical Estimates
"""
## Comparison: Optimized vs Simple Historical

How much better are the optimized parameters compared to naive estimation?
"""

# Get simple historical estimates for comparison
from src.optimization import ExposureRiskEstimator

simple_estimator = ExposureRiskEstimator(
    universe=estimator.risk_estimator.universe
)

comparison_data = []
for exp_id in exposure_ids[:5]:  # Just compare a few
    # Optimized estimate
    opt_est = risk_estimates.get(exp_id)
    
    # Simple historical (if we can get it)
    try:
        simple_est = simple_estimator.estimate_volatility(
            exposure_id=exp_id,
            estimation_date=estimation_date,
            method='historical',
            lookback_days=252,  # 1 year naive
            frequency='daily'
        )
        
        comparison_data.append({
            'exposure': exp_id,
            'optimized_vol': opt_est.total_volatility if opt_est else None,
            'simple_vol': simple_est.volatility if simple_est else None,
            'difference': ((opt_est.total_volatility - simple_est.volatility) / 
                          simple_est.volatility * 100) if opt_est and simple_est else None
        })
    except Exception as e:
        print(f"Could not compare {exp_id}: {e}")

comp_df = pd.DataFrame(comparison_data)
if not comp_df.empty:
    comp_df.plot(x='exposure', y=['optimized_vol', 'simple_vol'], 
                kind='bar', figsize=(10, 6))
    plt.ylabel('Volatility')
    plt.title('Optimized vs Simple Historical Volatility Estimates')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
```

### 6. Export Results (30 min)

```python
# Cell 10: Export for Portfolio Optimization
"""
## Export Results for Portfolio Construction

Save the risk estimates in formats ready for portfolio optimization.
"""

import json
from pathlib import Path

output_dir = Path('../analysis_results')
output_dir.mkdir(exist_ok=True)

# 1. Export covariance matrix
cov_matrix = estimator.get_covariance_matrix(exposure_ids, estimation_date)
cov_df = pd.DataFrame(cov_matrix, index=exposure_ids, columns=exposure_ids)
cov_df.to_csv(output_dir / 'production_covariance_matrix.csv')
print(f"âœ… Saved covariance matrix to {output_dir / 'production_covariance_matrix.csv'}")

# 2. Export risk estimates summary
risk_summary_df.to_csv(output_dir / 'production_risk_estimates.csv', index=False)
print(f"âœ… Saved risk estimates to {output_dir / 'production_risk_estimates.csv'}")

# 3. Export complete optimization inputs as JSON
opt_inputs = estimator.get_optimization_ready_inputs(exposure_ids, estimation_date)

# Convert numpy arrays to lists for JSON serialization
json_inputs = {
    'estimation_date': estimation_date.isoformat(),
    'exposures': exposure_ids,
    'volatilities': opt_inputs['volatilities'],
    'expected_returns': opt_inputs['expected_returns'],
    'covariance_matrix': opt_inputs['covariance_matrix'].tolist(),
    'correlation_matrix': opt_inputs['correlation_matrix'].tolist()
}

with open(output_dir / 'production_optimization_inputs.json', 'w') as f:
    json.dump(json_inputs, f, indent=2)
print(f"âœ… Saved complete optimization inputs to {output_dir / 'production_optimization_inputs.json'}")

# 4. Create summary report
summary_report = {
    'generated_date': datetime.now().isoformat(),
    'estimation_date': estimation_date.isoformat(),
    'parameter_summary': estimator.get_parameter_summary(),
    'risk_statistics': {
        'avg_risk_premium_vol': risk_summary_df['risk_premium_vol'].mean(),
        'avg_total_vol': risk_summary_df['total_vol'].mean(),
        'avg_rp_percentage': risk_summary_df['rp_percentage'].mean(),
        'min_correlation': float(lower_triangle.min()),
        'max_correlation': float(lower_triangle.max()),
        'avg_correlation': float(lower_triangle.mean())
    },
    'exposures_analyzed': len(exposure_ids),
    'exposures_list': exposure_ids
}

with open(output_dir / 'production_risk_analysis_report.json', 'w') as f:
    json.dump(summary_report, f, indent=2)
print(f"âœ… Saved analysis report to {output_dir / 'production_risk_analysis_report.json'}")
```

```python
# Cell 11: Final Summary
"""
## Summary: Production Risk Estimates

Key findings from the production parameter optimization:
"""

print("ðŸŽ¯ PRODUCTION RISK ESTIMATES SUMMARY")
print("=" * 50)
print(f"\nðŸ“Š Risk Premium Analysis:")
print(f"  Average RP Volatility: {risk_summary_df['risk_premium_vol'].mean()*100:.2f}%")
print(f"  Average Total Volatility: {risk_summary_df['total_vol'].mean()*100:.2f}%")
print(f"  Average RP % of Total: {risk_summary_df['rp_percentage'].mean():.1f}%")

print(f"\nðŸ”— Correlation Analysis:")
print(f"  Average Correlation: {lower_triangle.mean():.3f}")
print(f"  Correlation Range: [{lower_triangle.min():.3f}, {lower_triangle.max():.3f}]")
print(f"  Matrix Condition Number: {np.linalg.cond(correlation_matrix):.2f}")

print(f"\nâœ… Production Readiness:")
print(f"  - Covariance matrix is positive definite")
print(f"  - All volatilities are positive")
print(f"  - Correlation matrix is well-conditioned")
print(f"  - Risk decomposition shows {risk_summary_df['rp_percentage'].mean():.0f}% risk is compensated")

print(f"\nðŸ“ Exported Files:")
print(f"  - production_covariance_matrix.csv")
print(f"  - production_risk_estimates.csv")
print(f"  - production_optimization_inputs.json")
print(f"  - production_risk_analysis_report.json")

print(f"\nðŸš€ Ready for portfolio optimization!")
```

## Success Criteria

1. **Production API Usage**
   - [ ] Demonstrates clean usage of `OptimizedRiskEstimator`
   - [ ] Shows parameter summary and configuration
   - [ ] Uses production interface methods correctly

2. **Risk Analysis Completeness**
   - [ ] Analyzes all 14 implementable exposures
   - [ ] Shows risk decomposition (RP vs uncompensated)
   - [ ] Validates correlation matrix properties
   - [ ] Compares asset classes and categories

3. **Visualization Quality**
   - [ ] Professional multi-panel visualizations
   - [ ] Clear comparison charts
   - [ ] Correlation heatmap and clustering
   - [ ] Risk decomposition breakdowns

4. **Export Readiness**
   - [ ] Covariance matrix in CSV format
   - [ ] Risk estimates summary table
   - [ ] Complete JSON for optimization
   - [ ] Analysis report with key metrics

5. **Insights Generated**
   - [ ] Confirms risk premium dominates (>90%)
   - [ ] Shows correlation structure makes sense
   - [ ] Validates optimization produced good parameters
   - [ ] Ready for portfolio construction

## Notes for Implementation

- Use the actual production parameters (not demo values)
- Handle any missing exposures gracefully
- Include both risk premium and total return estimates
- Make visualizations publication-quality
- Ensure all exports are properly formatted
- Add interpretation and insights throughout
