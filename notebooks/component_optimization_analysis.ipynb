{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Optimization Analysis\n",
    "\n",
    "Comprehensive analysis of the production parameter optimization results using reusable analysis tools.\n",
    "\n",
    "This notebook analyzes the optimal parameters generated from the production optimization run, providing insights into parameter selection, performance metrics, and optimization effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Analysis modules\n",
    "from src.analysis.parameter_analysis import (\n",
    "    ParameterAnalyzer, \n",
    "    load_parameters_from_yaml,\n",
    "    analyze_parameter_file\n",
    ")\n",
    "from src.visualization.optimization_analysis import (\n",
    "    OptimizationVisualizer,\n",
    "    create_optimization_summary\n",
    ")\n",
    "from src.analysis.optimization_statistics import (\n",
    "    OptimizationStatistics,\n",
    "    calculate_statistical_power\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Analysis tools loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Production Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the production optimization results\n",
    "optimal_params_path = '../config/optimal_parameters.yaml'\n",
    "\n",
    "try:\n",
    "    # Load parameters using our analysis tools\n",
    "    optimal_params = load_parameters_from_yaml(optimal_params_path)\n",
    "    print(f\"‚úÖ Loaded optimal parameters from {optimal_params_path}\")\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = ParameterAnalyzer(optimal_params)\n",
    "    print(\"‚úÖ Parameter analyzer initialized\")\n",
    "    \n",
    "    # Get summary\n",
    "    summary_df = analyzer.create_parameter_summary()\n",
    "    print(f\"‚úÖ Parameter summary created: {len(summary_df)} parameter sets\")\n",
    "    \n",
    "    # Check if optimization failed\n",
    "    failed_count = len(summary_df[summary_df['optimization_status'] == 'failed'])\n",
    "    success_count = len(summary_df[summary_df['optimization_status'] == 'success'])\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"‚ö†Ô∏è Warning: {failed_count} parameter sets failed optimization\")\n",
    "        print(f\"üìä Status: {success_count} successful, {failed_count} failed\")\n",
    "        print(\"\\nNote: This analysis will demonstrate the framework with the available parameter structure\")\n",
    "        print(\"The optimization appears to have failed - this analysis shows the framework capabilities\")\n",
    "    else:\n",
    "        print(f\"‚úÖ All {success_count} parameter sets optimized successfully\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Optimal parameters file not found\")\n",
    "    print(\"Please ensure the production optimization has been run\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading parameters: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter Summary Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parameter summary\n",
    "print(\"üîç PARAMETER SUMMARY OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic counts\n",
    "component_counts = summary_df['component'].value_counts()\n",
    "print(f\"\\nComponent Distribution:\")\n",
    "for component, count in component_counts.items():\n",
    "    print(f\"  {component.replace('_', ' ').title()}: {count} parameter sets\")\n",
    "\n",
    "# Unique exposures\n",
    "unique_exposures = summary_df[summary_df['exposure_id'] != 'ALL']['exposure_id'].nunique()\n",
    "print(f\"\\nUnique Exposures: {unique_exposures}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìä Sample Parameter Data:\")\n",
    "display_cols = ['exposure_id', 'component', 'method', 'lookback_days', 'frequency', 'score']\n",
    "print(summary_df[display_cols].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method distribution analysis\n",
    "method_dist = analyzer.get_method_distribution()\n",
    "\n",
    "print(\"\\nüîç METHOD DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for component, methods in method_dist.items():\n",
    "    print(f\"\\n{component.replace('_', ' ').title()}:\")\n",
    "    for method, count in methods.items():\n",
    "        percentage = (count / sum(methods.values())) * 100\n",
    "        print(f\"  {method}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookback statistics\n",
    "lookback_stats = analyzer.get_lookback_statistics()\n",
    "\n",
    "print(\"\\nüìà LOOKBACK PERIOD ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "lookback_df = pd.DataFrame(lookback_stats).T\n",
    "if not lookback_df.empty:\n",
    "    print(lookback_df.round(1).to_string())\n",
    "else:\n",
    "    print(\"No lookback data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = OptimizationVisualizer(analyzer)\n",
    "\n",
    "# Create method distribution plot\n",
    "fig_methods = visualizer.plot_method_distribution(figsize=(14, 6))\n",
    "plt.suptitle('Production Optimization: Method Selection by Component', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Method distribution visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookback distribution analysis\n",
    "fig_lookback = visualizer.plot_lookback_distribution(figsize=(14, 10))\n",
    "plt.suptitle('Production Optimization: Lookback Period Distribution', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Lookback distribution visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score analysis\n",
    "fig_scores = visualizer.plot_score_analysis(figsize=(14, 10))\n",
    "plt.suptitle('Production Optimization: Score Analysis', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Score analysis visualization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter heatmap\n",
    "fig_heatmap = visualizer.plot_parameter_heatmap(figsize=(16, 12))\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Parameter heatmap visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard\n",
    "interactive_fig = visualizer.create_interactive_dashboard()\n",
    "interactive_fig.show()\n",
    "\n",
    "print(\"‚úÖ Interactive dashboard created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize statistical analyzer\n",
    "stats_analyzer = OptimizationStatistics(analyzer, confidence_level=0.95)\n",
    "\n",
    "print(\"üî¨ STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate comprehensive statistical report\n",
    "statistical_report = stats_analyzer.generate_statistical_report()\n",
    "\n",
    "print(\"‚úÖ Statistical analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component comparison\n",
    "component_comparison = statistical_report['component_comparison']\n",
    "\n",
    "print(\"\\nüìä COMPONENT PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'component_summaries' in component_comparison:\n",
    "    comp_summary_df = pd.DataFrame(component_comparison['component_summaries']).T\n",
    "    print(comp_summary_df.round(4).to_string())\n",
    "\n",
    "# ANOVA results\n",
    "if 'anova_result' in component_comparison and 'error' not in component_comparison['anova_result']:\n",
    "    anova = component_comparison['anova_result']\n",
    "    print(f\"\\nüî¨ ANOVA Test Results:\")\n",
    "    print(f\"  F-statistic: {anova['f_statistic']:.4f}\")\n",
    "    print(f\"  P-value: {anova['p_value']:.6f}\")\n",
    "    print(f\"  Significant: {anova['significant']}\")\n",
    "    print(f\"  Effect size (Œ∑¬≤): {anova['eta_squared']:.4f}\")\n",
    "    print(f\"  Interpretation: {anova['interpretation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robustness analysis\n",
    "print(\"\\nüõ°Ô∏è ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "robustness_results = statistical_report['robustness_analysis']\n",
    "\n",
    "for component, robustness in robustness_results.items():\n",
    "    if 'robustness_score' in robustness:\n",
    "        print(f\"\\n{component.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Robustness Score: {robustness['robustness_score']:.0f}/100\")\n",
    "        print(f\"  Interpretation: {robustness['interpretation']}\")\n",
    "        \n",
    "        if 'method_diversity' in robustness:\n",
    "            diversity = robustness['method_diversity']\n",
    "            print(f\"  Method Diversity: {diversity['unique_methods']} unique methods\")\n",
    "            print(f\"  Dominant Method: {diversity['dominant_method']} ({diversity['dominant_method_pct']:.1f}%)\")\n",
    "        \n",
    "        if 'score_robustness' in robustness:\n",
    "            score_rob = robustness['score_robustness']\n",
    "            print(f\"  Score Range: {score_rob['score_range']:.6f}\")\n",
    "            print(f\"  Score Skewness: {score_rob['score_skewness']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parameter Consistency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter consistency analysis\n",
    "consistency = analyzer.get_parameter_consistency()\n",
    "\n",
    "print(\"üéØ PARAMETER CONSISTENCY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "consistency_df = pd.DataFrame(consistency).T\n",
    "if not consistency_df.empty:\n",
    "    print(consistency_df.to_string())\n",
    "    \n",
    "    print(\"\\nüìã Consistency Summary:\")\n",
    "    for component, metrics in consistency.items():\n",
    "        print(f\"\\n{component.replace('_', ' ').title()}:\")\n",
    "        \n",
    "        if metrics['method_consistency']:\n",
    "            print(f\"  ‚úÖ Methods consistent: {metrics['dominant_method']}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Methods inconsistent (dominant: {metrics['dominant_method']})\")\n",
    "        \n",
    "        if metrics['frequency_consistency']:\n",
    "            print(f\"  ‚úÖ Frequencies consistent: {metrics['dominant_frequency']}\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Frequencies inconsistent (dominant: {metrics['dominant_frequency']})\")\n",
    "        \n",
    "        if not pd.isna(metrics['lookback_cv']):\n",
    "            cv = metrics['lookback_cv']\n",
    "            if cv < 0.1:\n",
    "                print(f\"  ‚úÖ Lookback periods very stable (CV: {cv:.3f})\")\n",
    "            elif cv < 0.3:\n",
    "                print(f\"  üìä Lookback periods moderately stable (CV: {cv:.3f})\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è Lookback periods variable (CV: {cv:.3f})\")\n",
    "else:\n",
    "    print(\"No consistency data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exposure-Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific exposures\n",
    "key_exposures = ['us_large_equity', 'dynamic_global_bonds', 'commodities', 'real_estate']\n",
    "\n",
    "print(\"üéØ EXPOSURE-SPECIFIC ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter for key exposures that exist in our data\n",
    "available_exposures = summary_df['exposure_id'].unique()\n",
    "analysis_exposures = [exp for exp in key_exposures if exp in available_exposures]\n",
    "\n",
    "if analysis_exposures:\n",
    "    comparison_df = analyzer.compare_exposure_parameters(analysis_exposures)\n",
    "    \n",
    "    print(f\"\\nComparing parameters for: {', '.join(analysis_exposures)}\")\n",
    "    print(\"\\nüìä Parameter Comparison:\")\n",
    "    \n",
    "    # Display the comparison in a readable format\n",
    "    for metric in ['method', 'lookback_days', 'frequency', 'score']:\n",
    "        if metric in comparison_df.columns.get_level_values(0):\n",
    "            print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "            metric_data = comparison_df[metric]\n",
    "            print(metric_data.to_string())\n",
    "else:\n",
    "    print(\"Key exposures not found in optimization results\")\n",
    "    print(f\"Available exposures: {list(available_exposures)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "print(\"üîç OUTLIER DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "outlier_results = {}\n",
    "\n",
    "for component in ['volatility', 'expected_returns']:\n",
    "    print(f\"\\n{component.replace('_', ' ').title()} Component:\")\n",
    "    \n",
    "    score_outliers = analyzer.identify_outliers(component, 'score')\n",
    "    lookback_outliers = analyzer.identify_outliers(component, 'lookback_days')\n",
    "    \n",
    "    outlier_results[component] = {\n",
    "        'score_outliers': score_outliers,\n",
    "        'lookback_outliers': lookback_outliers\n",
    "    }\n",
    "    \n",
    "    if score_outliers:\n",
    "        print(f\"  üìä Score outliers: {', '.join(score_outliers)}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ No score outliers detected\")\n",
    "    \n",
    "    if lookback_outliers:\n",
    "        print(f\"  üìà Lookback outliers: {', '.join(lookback_outliers)}\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ No lookback outliers detected\")\n",
    "\n",
    "# Summary\n",
    "total_outliers = sum(len(results['score_outliers']) + len(results['lookback_outliers']) \n",
    "                    for results in outlier_results.values())\n",
    "print(f\"\\nüìã Total outliers detected: {total_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optimization Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights\n",
    "insights = analyzer.generate_optimization_insights()\n",
    "\n",
    "print(\"üí° OPTIMIZATION INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display recommendations\n",
    "if insights['recommendations']:\n",
    "    print(\"\\nüéØ Key Recommendations:\")\n",
    "    for i, rec in enumerate(insights['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No specific recommendations - optimization appears well-configured\")\n",
    "\n",
    "# Additional insights from statistical analysis\n",
    "if statistical_report['recommendations']:\n",
    "    print(\"\\nüî¨ Statistical Recommendations:\")\n",
    "    for i, rec in enumerate(statistical_report['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "print(\"üìã PRODUCTION OPTIMIZATION SUMMARY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic statistics\n",
    "total_params = len(summary_df)\n",
    "unique_exposures = summary_df[summary_df['exposure_id'] != 'ALL']['exposure_id'].nunique()\n",
    "components = summary_df['component'].nunique()\n",
    "\n",
    "print(f\"\\nüìä Optimization Overview:\")\n",
    "print(f\"  Total parameter sets: {total_params}\")\n",
    "print(f\"  Unique exposures: {unique_exposures}\")\n",
    "print(f\"  Components optimized: {components}\")\n",
    "\n",
    "# Method distribution summary\n",
    "print(f\"\\nüîß Method Selection:\")\n",
    "all_methods = summary_df['method'].value_counts()\n",
    "for method, count in all_methods.items():\n",
    "    percentage = (count / len(summary_df)) * 100\n",
    "    print(f\"  {method}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Score summary\n",
    "scores = summary_df['score'].dropna()\n",
    "if len(scores) > 0:\n",
    "    print(f\"\\nüìà Score Analysis:\")\n",
    "    print(f\"  Mean score: {scores.mean():.6f}\")\n",
    "    print(f\"  Score range: {scores.min():.6f} - {scores.max():.6f}\")\n",
    "    print(f\"  Score std dev: {scores.std():.6f}\")\n",
    "\n",
    "# Lookback summary\n",
    "lookbacks = summary_df['lookback_days'].dropna()\n",
    "if len(lookbacks) > 0:\n",
    "    print(f\"\\nüìÖ Lookback Period Analysis:\")\n",
    "    print(f\"  Mean lookback: {lookbacks.mean():.1f} days\")\n",
    "    print(f\"  Lookback range: {lookbacks.min():.0f} - {lookbacks.max():.0f} days\")\n",
    "    print(f\"  Most common: {lookbacks.mode().iloc[0]:.0f} days\")\n",
    "\n",
    "# Robustness summary\n",
    "robust_components = sum(1 for comp, rob in robustness_results.items() \n",
    "                       if 'robustness_score' in rob and rob['robustness_score'] >= 80)\n",
    "total_components = len(robustness_results)\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è Robustness Assessment:\")\n",
    "print(f\"  Highly robust components: {robust_components}/{total_components}\")\n",
    "\n",
    "# Final assessment\n",
    "print(f\"\\nüéØ Overall Assessment:\")\n",
    "if total_outliers == 0 and robust_components == total_components:\n",
    "    print(\"  ‚úÖ EXCELLENT: Optimization is highly robust with consistent parameters\")\n",
    "elif total_outliers <= 2 and robust_components >= total_components * 0.8:\n",
    "    print(\"  üìä GOOD: Optimization is generally robust with minor inconsistencies\")\n",
    "elif total_outliers <= 5 or robust_components >= total_components * 0.6:\n",
    "    print(\"  ‚ö†Ô∏è MODERATE: Optimization shows some variability that may need attention\")\n",
    "else:\n",
    "    print(\"  ‚ùå NEEDS REVIEW: Optimization shows significant variability across components\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis complete! Review visualizations and recommendations above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results\n",
    "export_path = '../analysis_results'\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "# Export parameter summary\n",
    "summary_export_path = f'{export_path}/parameter_summary.csv'\n",
    "summary_df.to_csv(summary_export_path, index=False)\n",
    "print(f\"üìÑ Parameter summary exported to: {summary_export_path}\")\n",
    "\n",
    "# Export insights as JSON\n",
    "import json\n",
    "insights_export_path = f'{export_path}/optimization_insights.json'\n",
    "with open(insights_export_path, 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    insights_serializable = {}\n",
    "    for key, value in insights.items():\n",
    "        if key == 'parameter_summary':\n",
    "            continue  # Skip DataFrame\n",
    "        insights_serializable[key] = value\n",
    "    \n",
    "    json.dump(insights_serializable, f, indent=2, default=str)\n",
    "print(f\"üí° Optimization insights exported to: {insights_export_path}\")\n",
    "\n",
    "# Save visualizations\n",
    "viz_path = f'{export_path}/visualizations'\n",
    "figures = visualizer.create_summary_report(viz_path)\n",
    "print(f\"üìä Visualizations saved to: {viz_path}/\")\n",
    "\n",
    "print(\"\\n‚úÖ All analysis results exported successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
